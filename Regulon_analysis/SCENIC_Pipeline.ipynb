{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, glob\n",
    "import pickle\n",
    "import loompy\n",
    "\n",
    "from arboreto.utils import load_tf_names\n",
    "from arboreto.algo import grnboost2\n",
    "\n",
    "from pyscenic.rnkdb import FeatherRankingDatabase as RankingDatabase\n",
    "from pyscenic.utils import modules_from_adjacencies\n",
    "from pyscenic.prune import prune2df, df2regulons, _distributed_calc\n",
    "from pyscenic.aucell import aucell\n",
    "from dask.diagnostics import ProgressBar\n",
    "from distributed import LocalCluster, Client\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set locations to resource and database folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES_FOLDER=\"/pySCENIC/resources/\" #location to store motifs and transcription factor tables/lists. get from pyscenic github repo\n",
    "MOTIF_ANNOTATIONS_FNAME = os.path.join(RESOURCES_FOLDER, \"motifs-v9-nr.hgnc-m0.001-o0.0.tbl\")\n",
    "HS_TFS_FNAME = os.path.join(RESOURCES_FOLDER, 'hs_hgnc_tfs.txt')\n",
    "\n",
    "DATABASE_FOLDER = \"/pySCENIC/databases/\" #location to store feather databases, from https://resources.aertslab.org/cistarget/\n",
    "DATABASES_GLOB = os.path.join(DATABASE_FOLDER, \"hg19-*.mc9nr.feather\") #only three from the 10 species comparison are needed, from hg19, mc9nr for humans\n",
    "\n",
    "DATA_FOLDER=\".\" #output directory\n",
    "ADJACENCIES_FNAME = os.path.join(DATA_FOLDER, \"10_species_adj.csv\") #can change these output filenames, will output in DATA_FOLDER\n",
    "MODULES_FNAME = os.path.join(DATA_FOLDER, \"10_species_modules.p\") #can change these output filenames, will output in DATA_FOLDER\n",
    "MOTIFS_FNAME = os.path.join(DATA_FOLDER, \"10_species_motifs.csv\") #can change these output filenames, will output in DATA_FOLDER\n",
    "REGULONS_FNAME = os.path.join(DATA_FOLDER, \"10_species_regulons.p\") #can change these output filenames, will output in DATA_FOLDER\n",
    "AUC_FNAME = os.path.join(DATA_FOLDER, \"10_species_aucell.csv\") #can change these output filenames, will output in DATA_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load databases and TF info into environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatherRankingDatabase(name=\"hg19-tss-centered-5kb-10species.mc9nr\"),\n",
       " FeatherRankingDatabase(name=\"hg19-500bp-upstream-10species.mc9nr\"),\n",
       " FeatherRankingDatabase(name=\"hg19-tss-centered-10kb-10species.mc9nr\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_names = load_tf_names(HS_TFS_FNAME)\n",
    "db_fnames = glob.glob(DATABASES_GLOB)\n",
    "def name(fname):\n",
    "    return os.path.splitext(os.path.basename(fname))[0]\n",
    "dbs = [RankingDatabase(fname=fname, name=name(fname)) for fname in db_fnames]\n",
    "dbs #shows the 3 databases loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up dask cluster, remember to set proper port forwarding during SSH connection to forward the dashboard address to a local address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_cluster = LocalCluster(n_workers=2,threads_per_worker=32,dashboard_address=':2345',memory_limit='64GB') #Dashboard forwarded to port 2345\n",
    "custom_client_coexp =  Client(local_cluster)\n",
    "#Each n_worker added multiplies the memory_limit, but each thread per worker does not affect ram usage. \n",
    "#We have a total of 64 threads over 32 cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data in as data frame, set type to a 16 bit int to save memory, signed int16s have a max value of 32,767"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.read_h5ad(\"Discovery_Cohort_Counts.h5ad\").to_df().astype('int16') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run grnboost2 using the dask cluster, this takes a long time but you can check the progress using the dask dashboard address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing dask client\n",
      "parsing input\n",
      "creating dask graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bob/anaconda3/envs/scenic0103/lib/python3.8/site-packages/arboreto/algo.py:214: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  expression_matrix = expression_data.as_matrix()\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "adj = grnboost2(data, tf_names=tf_names, verbose=True,client_or_address=custom_client_coexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj.to_csv(ADJACENCIES_FNAME, index=False, sep='\\t') #Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Close cluster\n",
    "local_cluster.close()\n",
    "custom_client_coexp.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate modules and save as a .p file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-04-13 10:45:40,180 - pyscenic.utils - INFO - Calculating Pearson correlations.\n",
      "\n",
      "2021-04-13 10:45:41,819 - pyscenic.utils - WARNING - Note on correlation calculation: the default behaviour for calculating the correlations has changed after pySCENIC verion 0.9.16. Previously, the default was to calculate the correlation between a TF and target gene using only cells with non-zero expression values (mask_dropouts=True). The current default is now to use all cells to match the behavior of the R verision of SCENIC. The original settings can be retained by setting 'rho_mask_dropouts=True' in the modules_from_adjacencies function, or '--mask_dropouts' from the CLI.\n",
      "\tDropout masking is currently set to [True].\n",
      "\n",
      "2021-04-13 10:49:22,108 - pyscenic.utils - INFO - Creating modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 55min 9s, sys: 1h 42min 39s, total: 3h 37min 49s\n",
      "Wall time: 5min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modules = list(modules_from_adjacencies(adj, data, rho_mask_dropouts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODULES_FNAME, \"wb\") as f: #save as pickle\n",
    "    pickle.dump(modules, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a good stopping point if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in modules .p file from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODULES_FNAME, 'rb') as f: #read in modules file\n",
    "    modules = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load databases and TF info into environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatherRankingDatabase(name=\"hg19-tss-centered-5kb-10species.mc9nr\"),\n",
       " FeatherRankingDatabase(name=\"hg19-500bp-upstream-10species.mc9nr\"),\n",
       " FeatherRankingDatabase(name=\"hg19-tss-centered-10kb-10species.mc9nr\")]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_names = load_tf_names(HS_TFS_FNAME)\n",
    "db_fnames = glob.glob(DATABASES_GLOB)\n",
    "def name(fname):\n",
    "    return os.path.splitext(os.path.basename(fname))[0]\n",
    "dbs = [RankingDatabase(fname=fname, name=name(fname)) for fname in db_fnames]\n",
    "dbs #shows the 3 databases loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next step is parallelized through multiprocess and not dask, memory usage scales directly with the number of workers used and will crash if it runs out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 56min 57.8s\n",
      "CPU times: user 2min 5s, sys: 24.1 s, total: 2min 29s\n",
      "Wall time: 57min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LOGGER = logging.getLogger('pyscenic')\n",
    "LOGGER.setLevel(logging.CRITICAL)\n",
    "with ProgressBar(): \n",
    "    df = prune2df(dbs, modules, MOTIF_ANNOTATIONS_FNAME,module_chunksize=50,num_workers=2)\n",
    "#60K cells need 1 process\n",
    "#30K cells need 2 processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the enriched motifs and the discovered regulons to disk.\n",
    "df.to_csv(MOTIFS_FNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate regulons and save as a .p file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create regulons from a dataframe of enriched features.\n",
      "Additional columns saved: []\n"
     ]
    }
   ],
   "source": [
    "#convert to regulons\n",
    "regulons = df2regulons(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pickle file of regulons\n",
    "with open(REGULONS_FNAME, \"wb\") as f:\n",
    "    pickle.dump(regulons, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is a good stopping point if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase III"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in regulons .p file and count matrix from before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(REGULONS_FNAME, 'rb') as f:\n",
    "    regulon = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.read_h5ad(\"Discovery_Cohort_Counts.h5ad\").to_df().astype('int16') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run AUCell, may also take up a lot of RAM but is generally fast, may have to filter the input matrix in some cases if too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_mtx = aucell(data, regulons, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the enriched motifs and the discovered regulons to disk.\n",
    "auc_mtx.to_csv(AUC_FNAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qc_pipe",
   "language": "python",
   "name": "qc_pipe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
